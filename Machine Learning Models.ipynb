from google.colab import drive

drive.mount('/content/drive')

import pandas as pd
import glob
import numpy as np

fallArr = []
labelArr = []

def aggregate_features(data):
    # Extract statistical features for each column
    features = []
    for col in data.columns:
        data[col] = pd.to_numeric(data[col], errors='coerce')
        features.append(data[col].mean())  # Mean
        features.append(data[col].std())   # Standard deviation
        features.append(data[col].min())  # Minimum
        features.append(data[col].max())  # Maximum
    return np.array(features)

for file in glob.glob('/content/drive/My Drive/Lumiere/Fall_Test/*') :

  data = pd.read_csv(file, delimiter = " ", header = None, skiprows = 204)
  data = data[0:-204]
  data.columns = ["ax", "ay", "az", "gx", "gy", "gz"]
  # search for the word "Last"
  for i in range(len(data)) :
    if data.iloc[i]["ax"] == "Last" :
      print(file)

  features = aggregate_features(data)

  fallArr.append(features)
  labelArr.append(1)

for file in glob.glob('/content/drive/My Drive/Lumiere/NonFall_Test/*') :

  if 'nf08' in file :
    continue

  data = pd.read_csv(file, delimiter = " ", header = None, skiprows = 204)
  data.columns = ["ax", "ay", "az", "gx", "gy", "gz"]
  data = data[0:-204]
  for i in range(len(data)) :
    if data.iloc[i]["ax"] == "Last" :
      print(file)
  # for col in data.columns:
  #   data[col] = pd.to_numeric(data[col], errors='coerce')
  features = aggregate_features(data)

  fallArr.append(features)
  labelArr.append(0)

from sklearn.ensemble import RandomForestClassifier
from sklearn import metrics
from sklearn.metrics import precision_score, recall_score, confusion_matrix
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.svm import SVC
import sklearn.tree

modelForest = RandomForestClassifier(n_estimators = 100)
# model = RandomForestClassifier(max_depth=3) everything above 2 returns the same value as without adding this, so it seems like adding max depth will not improve accuracy
fall_train, fall_test, label_train, label_test = train_test_split(fallArr, labelArr,
                                   random_state=777,
                                   test_size=0.3,
                                   shuffle=True,
                                   stratify=labelArr)
modelForest.fit(fall_train, label_train)

label_predicted_Forest = modelForest.predict(fall_test)

print("Forest data:\n")
print(metrics.accuracy_score(label_test, label_predicted_Forest))
print(precision_score(label_test, label_predicted_Forest))
print(recall_score(label_test, label_predicted_Forest))

print(confusion_matrix(label_test, label_predicted_Forest))

# high 90s accuracy, usually doesn't return false negatives, but does a decent amount of false positives

modelTree = sklearn.tree.DecisionTreeClassifier(criterion='gini',
                                                splitter='best',
                                                min_samples_split = 3,
                                                random_state = 777,
                                                )
modelTree.fit(fall_train, label_train)

label_predicted_Tree = modelTree.predict(fall_test)

print("\n\nTree data:\n\n")
print(metrics.accuracy_score(label_test, label_predicted_Tree))
print(precision_score(label_test, label_predicted_Tree))
print(recall_score(label_test, label_predicted_Tree))
print(confusion_matrix(label_test, label_predicted_Tree))

# still 90s accuracy, just everything a bit lower

modelLogistic = LogisticRegression(random_state=777, max_iter=1000)
modelLogistic.fit(fall_train, label_train)

label_predicted_Logistic = modelLogistic.predict(fall_test)

print("\nLogistic data:\n\n")
print(metrics.accuracy_score(label_test, label_predicted_Logistic))
print(precision_score(label_test, label_predicted_Logistic))
print(recall_score(label_test, label_predicted_Logistic))
print(confusion_matrix(label_test, label_predicted_Logistic))

# exact same data as the random forest model

modelNeighbors = KNeighborsClassifier(n_neighbors=6)
modelNeighbors.fit(fall_train, label_train)

label_predicted_Neighbors = modelNeighbors.predict(fall_test)

print("\nNeighbors data:\n\n")
print(metrics.accuracy_score(label_test, label_predicted_Neighbors))
print(precision_score(label_test, label_predicted_Neighbors))
print(recall_score(label_test, label_predicted_Neighbors))
print(confusion_matrix(label_test, label_predicted_Neighbors))

# pretty good, almost as gooed as forest and logistic regression

modelSVM = SVC(kernel='linear', random_state=777)
modelSVM.fit(fall_train, label_train)

label_predicted_SVM = modelSVM.predict(fall_test)

print("\nSVM data:\n\n")
print(metrics.accuracy_score(label_test, label_predicted_SVM))
print(precision_score(label_test, label_predicted_SVM))
print(recall_score(label_test, label_predicted_SVM))
print(confusion_matrix(label_test, label_predicted_SVM))

# same accuracy as neighbors but with higher lower precision and hgher recall

import glob
import pandas as pd
import numpy as np

# List to store all `ax` arrays
all_ax = []

# Read every file and process each one
for file in glob.glob('/content/drive/My Drive/Lumiere/Fall_Test/*'):
    data = pd.read_csv(file, delimiter=" ", header=None, skiprows=204)
    data = data[0:-204]  # Trimming rows, if necessary
    data.columns = ["ax", "ay", "az", "gx", "gy", "gz"]

    # Convert 'ax' column to numeric, coercing errors to NaN
    numeric_ax = pd.to_numeric(data['ax'], errors='coerce')  # Coerce invalid values to NaN
    all_ax.append(numeric_ax.dropna().tolist())  # Append the cleaned numeric list

# Ensure all arrays are the same length by truncating or padding with NaN
max_length = max(len(arr) for arr in all_ax)
normalized_ax = np.array([np.pad(arr, (0, max_length - len(arr)), constant_values=np.nan) for arr in all_ax])

# Compute the average at each point (ignoring NaNs)
average_ax = np.nanmean(normalized_ax, axis=0)


plt.plot(average_ax)
